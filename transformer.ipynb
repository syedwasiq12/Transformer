{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Creating Embeddings"
      ],
      "metadata": {
        "id": "4nPcfP8yQz4S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKS8tEvnQzBy",
        "outputId": "0387b6a0-8d11-482f-c5bb-d7d8fefed788"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "pip install torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kcmE0CWlSVaS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "class Embeddings(nn.Module):\n",
        "  def __init__(self,seq_length,d_model,vocab_size):\n",
        "    super().__init__()\n",
        "    self.vocab_size=vocab_size\n",
        "    self.seq_length=seq_length\n",
        "    self.d_model=d_model\n",
        "    self.embeddings=nn.Embeddings(self.vocab_size,self.d_model)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.embeddings(x) * (math.sqrt(self.d_model))  ##they multiplied the embeddings with the square root of d_model in the original paper\n",
        "\n",
        "class PosEmbeddings(nn.Module):\n",
        "  def __init__(self,seq_length,d_model,vocab_size,dropout):\n",
        "    super().__init__()\n",
        "    self.vocab_size=vocab_size\n",
        "    self.seq_length=seq_length\n",
        "    self.d_model=d_model\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "    # creating a matrix to store pos embeddings\n",
        "\n",
        "    pe=torch.zeros(self.seq_length,self.d_model)## pe=(seq_length x d_model)\n",
        "    pos=torch.arange(0,self.seq_length,dtype=int).unsqueeze(1)##(seq_length x 1), to get positions ## whenever we initialize tensors like torch.arange(0,y)\n",
        "    ## the dimemsion becomes (y,)by itself so we use unsqueeze to convert it to(y,1)\n",
        "    div_term=torch.exp((torch.arange(0,self.d_model,2))* (-math.log(1000))/self.d_model)\n",
        "\n",
        "    ##pe(pos,2i)=sin(pos/1000^(2i/d_model))\n",
        "    ##pe(pos,2i+1)=cos(pos/1000^(2i/d_model))\n",
        "\n",
        "    pe[:,0::2]=math.sin(pos*div_term)\n",
        "    pe[:,1::2]=math.cos(pos*div_term)\n",
        "\n",
        "    pe=pe.unsqueeze(0)   ##(batch_size,seq_length,d_model) is the dimension for it\n",
        "    self.register_buffer('pe',pe)\n",
        "\n",
        "\n",
        "  def forward(self,x):## x is the sentence\n",
        "    x=x=(self.pe[ : , :x[1], : ]).requires_grad_(False)  ##since pos embeddings are not to be trained so we keep requires_grad as False\n",
        "    return self.dropout(x)\n",
        "\n",
        "    ##the dropout layer is applied to the positional embeddings.\n",
        "    ##This helps prevent overfitting on positional information by slightly regularizing it, ensuring that the model doesn't rely too heavily on specific positions in the sequence\n"
      ],
      "metadata": {
        "id": "9Id8XhD8Q5-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Layer Norm"
      ],
      "metadata": {
        "id": "rSBnSEKca73O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class layer_norm(nn.Module):\n",
        "  def __init__(self,eps=1e-6):\n",
        "    super().__init__()\n",
        "    self.eps=eps\n",
        "    self.alpha=nn.Parameter(torch.ones(1)) ##initializing alpha and beta\n",
        "    self.beta=nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self,x):\n",
        "    mean=x.mean(dim=-1,keepdim=True)  ## dim=-1 means it is doing norm across last dimensions which is across features or d_model here\n",
        "    std=x.std(dim=-1,keepdim=True)\n",
        "    return self.alpha*((x-mean)/(std+self.eps))+self.beta"
      ],
      "metadata": {
        "id": "BBYbHO15R-cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feed Forward layer"
      ],
      "metadata": {
        "id": "mR8yuu76e6dT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ffnn(nn.Module):\n",
        "  def __init__(self,d_model,d_ff,dropout):  ## dropout is the parameter, how much of dropout we would want from 0-1\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.d_ff=d_ff\n",
        "    self.Linear_1=nn.Linear(d_model,d_ff)\n",
        "    self.Linear_2=nn.Linear(d_ff,d_model)\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.Linear_2(self.dropout(torch.relu(self.Linear_1(x))))"
      ],
      "metadata": {
        "id": "ac0WyWQnb2OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multihead Attention"
      ],
      "metadata": {
        "id": "EI4iTgrI_iE8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self,d_model,h : int,dropout):\n",
        "    super().__init__()\n",
        "    self.d_model=d_model\n",
        "    self.h=h\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.wq=nn.Linear(self.d_model,self.d_model)\n",
        "    self.wk=nn.Linear(self.d_model,self.d_model)\n",
        "    self.wv=nn.Linear(self.d_model,self.d_model)\n",
        "    self.wo=nn.Linear(self.d_model,self.d_model)\n",
        "\n",
        "    assert d_model%h==0\n",
        "    self.d_k=(d_model/h)\n",
        "\n",
        "    @staticmethod\n",
        "    def selfattention(query,key,value,mask,dropout):\n",
        "      d_k=query.shape[-1]\n",
        "      attention_scores=(query @ key.transpose(-2,-1))/math.sqrt(d_k)  ##(batch,h,seq_len,d_k)-->(batch,h,seq_len,seq_len)(dimension of attention scores),where d_k is the embedding dimension for each head\n",
        "      attention_scores=attention_scores.softmax(dim=-1)\n",
        "      if mask is not None:\n",
        "        attention_scores.masked_fill_(mask==0,-1e9)\n",
        "\n",
        "      if dropout is not None:\n",
        "        attention_scores=dropout(attention_scores)\n",
        "\n",
        "      return (attention_scores@value),attention_scores  ## dimension of ouptut for attention_scores@value will be (batch,h,seq_length,d_k) as attention score dim is (batch,h,seq,seq)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self,q,k,v,mask):\n",
        "      query=self.wq(q)   ##(batch,seq_len,d_model)-->(batch,seq_len,d_model)\n",
        "      key=self.wq(k)   ##(batch,seq_len,d_model)-->(batch,seq_len,d_model)\n",
        "      value=self.wq(v)   ##(batch,seq_len,d_model)-->(batch,seq_len,d_model)\n",
        "\n",
        "      query=query.view(query.shape[0],query.shape[1],self.h,self.d_k).transpose(1,2)  ##(batch,seq_len,d_model) -->(batch,seq_len,h,d_k) -->(batch,h,seq_len,d_k)\n",
        "      key=key.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)  ##(batch,seq_len,d_model) -->(batch,seq_len,h,d_k) -->(batch,h,seq_len,d_k)\n",
        "      value=value.view(key.shape[0],key.shape[1],self.h,self.d_k).transpose(1,2)  ##(batch,seq_len,d_model) -->(batch,seq_len,h,d_k) -->(batch,h,seq_len,d_k)\n",
        "\n",
        "      x,attention_scores=MultiheadAttention.selfattention(query,key,value,mask,self.dropout)\n",
        "\n",
        "      x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)  # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "\n",
        "      ##.contiguous() ensures that the tensorâ€™s memory layout is contiguous. After a transpose operation, the underlying memory might no longer be contiguous,\n",
        "      ##which can lead to issues if we try to reshape it. .contiguous() rearranges the memory to ensure the tensor is laid out contiguously, enabling the next view operation.\n",
        "      return self.w_o(x)\n"
      ],
      "metadata": {
        "id": "twkGzHD4f2Tz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Residual Connection"
      ],
      "metadata": {
        "id": "C9iSNuBHQvNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class residual(nn.Module):\n",
        "  def __init__(self,dropout):\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.norm=nn.LayerNorm()\n",
        "\n",
        "  def forward(self,x,sublayer):\n",
        "    return x+self.dropout(sublayer(self.norm(x)))  ##sublayer is the attention"
      ],
      "metadata": {
        "id": "N2s240I2QDtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Encoder block"
      ],
      "metadata": {
        "id": "kE2ArrV2Q40t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "  def __init__(self,x,self_attention_block : MultiheadAttention,feed_forward_block:ffnn, dropout:float):\n",
        "    super().__init()\n",
        "    self.attention_block=self_attention_block\n",
        "    self.feed_forward=feed_forward_block\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.residual=nn.ModuleList([residual(dropout) for _ in range(2)])   ## the reason for not defining residual in  the init method is that we need to apply it twice for which we use nn.ModuleLisy\n",
        "\n",
        "  def forward(self,x,src_mask):\n",
        "    x=self.residual[0](x,lambda x: self.attention_block(x,x,x,src_mask))  ## src mask here is for masking the paddings while training\n",
        "    x=self.residual[1](x,lambda x: self.feed_forward(x))\n",
        "    return x\n",
        "\n",
        "class Encoder(nn.Module):  ## since we have multiple attention blocks\n",
        "  def __init__(self,layers: nn.ModuleList):\n",
        "    super().__init()\n",
        "    self.layers=layers\n",
        "    self.layernorm=layer_norm()\n",
        "\n",
        "  def forward(self,mask):\n",
        "    for layer in self.layers:\n",
        "      x=layer(x,mask)\n",
        "    return self.norm(x)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SxxiJIGkQrih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decoder"
      ],
      "metadata": {
        "id": "n6Lcw2_RllZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "  def __init__(self,self_attention: MultiheadAttention, cross_attention: MultiheadAttention,feed_forward: ffnn, dropout:float):\n",
        "    super().__init()\n",
        "    self.self_attention=self_attention\n",
        "    self.cross_attention=cross_attention\n",
        "    self.feed_forward=feed_forward\n",
        "    self.dropout=nn.Dropout(dropout)\n",
        "    self.residual=nn.ModuleList([residual(dropout) for _ in range(3)])\n",
        "\n",
        "  def forward(self,x,encoder_output,src_mask,tgt_mask,dropout):  ##src mask is the mask from the encoder to be used in cross attention\n",
        "  ##and tgt mask is the mask of the decoder to be used in the self attention in decoder. Also x is the decoder input while as encoder_output is the encoder output\n",
        "    x=self.residual[0](x,lambda x: self.attention_block(x,x,x,tgt_mask))\n",
        "    x=self.residual[1](x, lambda x: self.cross_attention(x,encoder_output,encoder_output,src_mask))\n",
        "    x=self.residual[2](x, lambda x: self.feed_forward(x))\n",
        "    return x\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,layers : nn.ModuleList):\n",
        "    super().__init()\n",
        "    self.layers=layers\n",
        "    self.layer_norm=layer_norm\n",
        "\n",
        "  def forward(self,x,mask):\n",
        "    for layers in self.layers:\n",
        "      x=layers(x,mask)\n",
        "    return self.norm(x)  ##the output is (seq x d_model) if we don't use batch_size\n"
      ],
      "metadata": {
        "id": "pm15luP6pdIh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##now for the output we need to project it into the vocabulary for which we use the projection layer"
      ],
      "metadata": {
        "id": "F8XTklTcST87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final Projection layer"
      ],
      "metadata": {
        "id": "EN-VWeK0lnRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class proj_layer(nn.Module):\n",
        "  def __init__(self,d_model:int,vocab_size:int):\n",
        "    super().__init()\n",
        "    self.proj=nn.Linear(d_model,vocab_size)\n",
        "\n",
        "  def softmax(self,x):\n",
        "    ##(batch,seq_length,d_model)---->(batch,seq_length,vocab_size)\n",
        "    return torch.log_softmax(self.proj(x),dim=-1)\n",
        "\n",
        "##what basically happens here the output is of dimension (seq_length,d_model){not considering batch_size} and it gets converted to (seq_length,vocab_size),\n",
        "##so basically each token here is the combination of all the tokens in the vocabulary and after applying softmax on this we get the probability of each word in the vocabulary for this word"
      ],
      "metadata": {
        "id": "vcvwvQ1mTKHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformer"
      ],
      "metadata": {
        "id": "_bTahHYWpoFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):   ##(this is the generaline blueprint of transformere architecture, here no parameters are initialized or so)\n",
        "  def __init__(self,encoder: Encoder, decoder:Decoder, src_emb:Embeddings, tgt_emb: Embeddings,src_pos:  PosEmbeddings, tgt_pos:  PosEmbeddings, feedforward:ffnn, proj_layer:proj_layer  ):\n",
        "    super().__init()\n",
        "    self.src_embed=src_emb\n",
        "    self.tgt_emb=tgt_emb\n",
        "    self.src_pos=src_pos\n",
        "    self.tgt_pos=tgt_pos\n",
        "    self.encoder=encoder\n",
        "    self.decoder=decoder\n",
        "    self.proj_layer=proj_layer\n",
        "##the residual layer is already embedded in the encode and decode layers\n",
        "  def encode(self,src,src_mask):\n",
        "    src=self.src_embed(src)\n",
        "    src=self.src_pos(src)\n",
        "    return self.encoder(src,src_mask)\n",
        "\n",
        "  def decode(self,tgt,encoder_output,src_mask,tgt_mask):\n",
        "    tgt=self.tgt_emb(tgt)\n",
        "    tgt=self.tgt_pos(tgt)\n",
        "    return self.decoder(tgt,encoder_output,src_mask,tgt_mask)\n",
        "\n",
        "  def proj_layer(self,x):\n",
        "    return self.proj_layer(x)"
      ],
      "metadata": {
        "id": "WdP86xpvsDTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building a transformer now with parameters initialized"
      ],
      "metadata": {
        "id": "g5PXUxzxs2cn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_transformer(src,src_vocab_size:int,tgt_vocab_size: int,src_seq_length: int,tgt_seq_length: int,d_model: int=512,N: int=6, h: int=8, dropout:float=0.1,d_ff:int=2048):\n",
        "\n",
        "  ##the above are parameters to be defined for building the transformer N is the number of blocks of encoder/decoder stacked up while as h is the number of heads\n",
        "\n",
        "\n",
        "  ##creating the embedding layers\n",
        "  src_embed=Embeddings(d_model,src_vocab_size)\n",
        "  tgt_embed=Embeddings(d_model,tgt_vocab_size)\n",
        "\n",
        "  ##creating pos embeddings\n",
        "  src_pos=PosEmbeddings(d_model,src_vocab_size)\n",
        "  tgt_pos=PosEmbeddings(d_model,tgt_vocab_size)\n",
        "\n",
        "  encoder_blocks=[]\n",
        "  for encoder in range(N):\n",
        "    encoder_self_attention=MultiheadAttention(d_model,h,dropout)\n",
        "    encoder_feed_forward=ffnn(d_ff ,d_ff,dropout)\n",
        "    encoder_block=EncoderBlock( encoder_self_attention, encoder_feed_forward,dropout)\n",
        "    encoder_blocks.append(encoder)\n",
        "    ##all the things in these structures that depend on the parameters are initialized here like encoder depends on d_model,h,dropout\n",
        "\n",
        "  decoder_blocks=[]\n",
        "  for encoder in range(N):\n",
        "    decoder_self_attention=MultiheadAttention(d_model,h,dropout)\n",
        "    decoder_cross_attention=MultiheadAttention(d_model,h,dropout)\n",
        "    decoder_feed_forward=ffnn(d_ff ,d_ff,dropout)\n",
        "    decoder=DecoderBlock( encoder_self_attention,decoder_cross_attention, encoder_feed_forward,dropout)\n",
        "    decoder_blocks.append(decoder)\n",
        "\n",
        "  encoder=Encoder(nn.ModuleList(encoder_blocks))\n",
        "  decoder=Decoder(nn.ModuleList(decoder_blocks))\n",
        "  projection_layer=proj_layer(d_model,tgt_vocab_size)\n",
        "\n",
        "  transformer=Transformer(encoder,decoder,src_embed,tgt_embed,src_pos,tgt_pos, PosEmbeddings, projection_layer)\n",
        "\n",
        "  for p in transformer.parameter:\n",
        "    if p.dim()>1:\n",
        "      nn.init.xavier_uniform_(p)\n",
        "## initializing the parameters using Xavier initialization to get faster training\n",
        "  return transformer"
      ],
      "metadata": {
        "id": "lJjJz3b4s7wO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we are gonna train this for a translation task using the huggingface dataset"
      ],
      "metadata": {
        "id": "8c_VKl6whDDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "wtAu09NX5wEz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}